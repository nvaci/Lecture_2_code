---
title: "Lecture2_Rcode"
author: "Nemanja Vaci"
date: "February 16, 2021"
output: html_document
---

We can also simulate categorical outcomes.

```{r}
set.seed(456)
Babies=data.frame(Age=round(runif(100,1,30)), Weight=rnorm(100,4000,500))
Babies$Height=rnorm(100,40+0.2*Babies$Age+0.004*Babies$Weight, 5)
Babies$Gender=rbinom(100,1,0.5)
Babies$Crawl=rbinom(100,1,0.031*Babies$Age+0.00001*Babies$Weight-0.06*Babies$Gender) #I simulated Crawling data from random binomial distribution, where I took out 100 times 1 sample. Probability of success was defined in relation to Babies Age, Weigh and Gender. 
Babies$Gender=as.factor(Babies$Gender) # I recode these numbers to factor
levels(Babies$Gender)=c('Girls','Boys') # Assigning labels to Gender factor
table(Babies$Crawl)
```

Plotting probability density function for Number of babies crawling in our data 

```{r,fig.width=14, fig.height=5, fig.align='right'}
par(mfrow=c(1,1), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(1:100,dbinom(1:100,0.63, size=100), xlab='N of babies crawling', ylab='Probability', type='l')
```

We can also calculate the probability of drawing exactly 75 babies that crawl from our Binomial distribution.

```{r}
x=75
choose(100, x)*0.63^x*(1-0.63)^(100-x) #N chose X - gives us how many ways combinations of random binomial drawings result in exactly 75 babies that crawl: https://en.wikipedia.org/wiki/Combination 
```

What is the probability of drawing _at least_ 64 babies based on our probability distribution: 

```{r}
x=64:100
sum(choose(100, x)*0.63^x*(1-0.63)^(100-x))
```

Lets simulate some logit values and transform them back to odds and probabilities

```{r, message=FALSE, fig.width=12, fig.height=5, fig.align='center'}
require(ggplot2) #load in ggplot2
logit<-data.frame(LogOdds=seq(-2.5,2.5, by=.1), Pred=seq(-2.5,2.5, by=.1)) #create data frame where variable LogOdds containts numbers from -2.5 to 2.5 changing by 0.1 
logit$Odds=exp(logit$LogOdds) #exponentiate logits that results in odds
logit$Probabilities=logit$Odds/(1+logit$Odds) #transform odds ratios to probabilities 

ggplot(data = logit, aes(x=Pred, y=Odds))+geom_point(size=2)+theme_bw()+ylim(0,13)+theme(axis.title=element_text(size=14), axis.text =element_text(size=12)) #plotting odds
```

Plotting Log-Odds: 

```{r}
ggplot(data = logit, aes(x=Pred, y=LogOdds))+geom_point(size=2)+theme_bw()+ylim(-4,4)+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```

Plotting probabilities:

```{r}
ggplot(data = logit, aes(x=Pred, y=Probabilities))+geom_point(size=2)+theme_bw()+ylim(0,1)+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```

We can again check what is in our simulated data:

```{r}
par(mfrow=c(1,3), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(Babies$Age, Babies$Height, xlab='Age (months)', ylab='Height (cm)')
boxplot(Babies$Height~Babies$Gender,xlab='Gender', ylab='Height (cm)')
boxplot(Babies$Age~Babies$Crawl,xlab='Crawl', ylab='Age (months)') #Boxplot of Babies Age across our Crawling outcome, xlab - label of x-axis, ylab - label of y-axis
```

Let's build first logistic regression model:

```{r}
glm1<-glm(Crawl~Age, data=Babies, family=binomial(link='logit')) #glm function is used to fit generalized linear models (try typing in your console: ?glm). Crawl is modelled as a function of Age and we are using Babies data. Family specified type of the outcome distribution. We are saying that this our oucome follows Binomial distribution, while we want to use logit link - logOdds transformation. 
glm1$coefficients #we are printing only coefficients
```

Let's get Odds ratios:

```{r}
glm1$coefficients
exp(glm1$coefficients) #we can use exponential function to transform our coefficients to Odds ratios - how more likely it is that babies are going to start crawling if they are older by 1 month (beta coefficient - slope)
```

Let's get probabilities:

```{r}
1/(1+exp(1.33078)) # only intercept
1/(1+exp(1.33078-0.11948*10)) #What is the probability of babies starting to crawl when they are 10 months?
arm::invlogit(coef(glm1)[[1]]+coef(glm1)[[2]]*mean(Babies$Age)) # what is the probability of babies starting to crawl when they are mean of their age (around 16 months). I used invlogit function to automatically calculate probabilities. coef(glm1)[[1]] - gives me intercept value, while coef(glm1)[[2]] - gives me slope value for age.
```

We can plot everything inner workings of our model: Logit, odds and probabilities 

```{r}
Babies$LogOdds=-1.33078+0.11948*Babies$Age #Outputing logit values based on our model
Babies$Odds=exp(Babies$LogOdds) #Transforming them to odds 
Babies$Probs=Babies$Odds/(1+Babies$Odds) # Transforming odds to probabilities
par(mfrow=c(1,3), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(Babies$Age,Babies$LogOdds)
plot(Babies$Age, Babies$Odds)
plot(Babies$Age,Babies$Probs)
```

Lets see what goes into the model and how our model sees the data:

```{r}
ggplot(data=Babies, aes(x=Age, y=Probs))+geom_point(size=2, col='blue')+geom_point(data=Babies, aes(x=Age, y=Crawl), size=2, col='red') #Red points show our dependent outcome - 0,1; blue points show estimated probabilities across the values of Age. 
```

```{r}
summary(glm1) #Summary of the complete model
```

We can calculate whether our proposed model is improvement in comparison to the null (only intercept model) - statistically significant:

```{r}
with(glm1, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) #I am asking R to take values from my glm1 object (our model), where I am asking him to take p-values from chi-square distribution where my values are difference between null deviance model and our my fitted model, differences in the degrees of freedom, and we are looking at the right side of the probability distribution with lower.tail=FALSE
```

Practical aspect:

```{r}
basketball<-read.table('Basketball.txt',sep='\t', header=T) #Loading the data in R. Data file is .txt file, where separator is tab delimited (tab delimited values) and the names of my variables are in the first row (header=T)
knitr::kable(head(basketball[,c(5,13,18,31,34,43)]), format = 'html') #Printing only certain columns (numbers in c())
```

Let's plot the data to see how it looks like:

```{r, fig.width=12, fig.height=5, fig.align='center'}
table(basketball$Win) #Tabulate outcome
par(mfrow=c(1,2), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(density(basketball$X3PointShots.), main='', xlab='3POintsShots')
plot(density(basketball$Opp.3PointShots.), main='', xlab='Opp3PointsShots')
```

We can also cross-tabulate the data. Focus on combinations of categories:

```{r}
knitr::kable(table(basketball$Win, basketball$Home), format = 'html')
datA=aggregate(cbind(basketball$X3PointShots., basketball$Opp.3PointShots., basketball$FreeThrows., basketball$Opp.FreeThrows.), list(basketball$Win), mean) #aggregate function aggregates our data. I used this function to calculate arithmetic mean for X3POintsShots, Opp3Points shots, FreeThrows, OppFreeThrows for each outcome value (0,1). cbind is used to join all numerical values together in one dataframe. In other words, we are not aggregating one by one numerical variable, as cbind allows us to do that jointly. Instead of mean, you can use sd or min or max or whatever you want.  
names(datA)=c('Win','X3POints_mean','Opp3POints_mean','FreeThrows_mean','OppFreeThrows_mean') #as my aggregate returns new data frame without the names of the variables I just quickly attach the names to them.
knitr::kable(datA, format = 'html') #Writting it out
```

Plotting predictors and categorical outcome

```{r, fig.width=12, fig.height=5, fig.align='center'}
par(mfrow=c(1,2), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(basketball$X3PointShots.,basketball$Win)
plot(basketball$X3PointShots.,jitter(basketball$Win, 0.5))
```

Let's make first model (one predictor):

```{r}
baskWL1<-glm(Win~Home, data=basketball, family=binomial('logit')) #Main effect of Home
summary(baskWL1)
```

Model with two predictors: 

```{r, size="tiny"}
baskWL2<-glm(Win~Home+X3PointShots., data=basketball, family=binomial('logit')) #Main effect of home and X3PointsShots.
summary(baskWL2)
```

Model comparison (model 2 versus model 1)

```{r}
anova(baskWL1, baskWL2, test = "LR") #compare two models where we use likelihood ratio test - for generalized linear models  
```

Three predictors:

```{r, size="tiny"}
baskWL3<-glm(Win~Home+X3PointShots.+Opp.3PointShots., data=basketball, family=binomial('logit')) #Main effect of Home, X3PointsShots. and Opp.3PointsShots.
anova(baskWL1,baskWL2, baskWL3, test = "LR") #comparing three models
```

Interactions:

```{r, size="tiny"}
baskWL4<-glm(Win~Home*X3PointShots.+Opp.3PointShots., data=basketball, family=binomial('logit')) #Three main effects and interaction between Home and X3PointsShots
anova(baskWL3, baskWL4, test = "LR") #comparing third and fourth model
```

Visualising our model with two predictors:

```{r,fig.width=12, fig.height=5, fig.align='center'}
basketball$Prob_mod2=predict(baskWL2, type='response')
ggplot(data=basketball, aes(x=X3PointShots., y=Prob_mod2, colour=Home))+geom_point(size=2)+geom_point(data=basketball, aes(x=X3PointShots., y=Win), size=2, col='blue')
```

Visualising our model with three predictors

```{r,fig.width=12, fig.height=5, fig.align='center'}
basketball$Prob_mod3=predict(baskWL3, type='response')
ggplot(data=basketball, aes(x=X3PointShots., y=Prob_mod3, colour=Home))+geom_point(size=2)+geom_point(data=basketball, aes(x=X3PointShots., y=Win), size=2, col='blue')
```

It is quite nice to report confidence intervals in your work:

```{r}
confint(baskWL3)#confidence intervals for logit estimates in our third model
exp(confint(baskWL3))#confidence intervals for odds ratios in our third model
```

Let's see how accurate is our model:

```{r}
Ctabs<-table(basketball$Win,basketball$Prob_mod3>0.5)
Ctabs
```